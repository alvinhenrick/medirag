# This file was autogenerated by uv via the following command:
#    uv pip compile pyproject.toml --output-file requirements.txt
accelerate==1.4.0
    # via medirag (pyproject.toml)
aiofiles==23.2.1
    # via gradio
aiohappyeyeballs==2.4.6
    # via aiohttp
aiohttp==3.11.12
    # via
    #   datasets
    #   fsspec
    #   huggingface-hub
    #   langchain
    #   litellm
    #   llama-index-core
    #   voyageai
aiolimiter==1.2.1
    # via voyageai
aiosignal==1.3.2
    # via aiohttp
alembic==1.14.1
    # via optuna
annotated-types==0.7.0
    # via pydantic
anyio==4.8.0
    # via
    #   asyncer
    #   dspy
    #   gradio
    #   httpx
    #   openai
    #   starlette
apscheduler==3.11.0
    # via litellm
asttokens==3.0.0
    # via stack-data
asyncer==0.0.8
    # via dspy
attrs==25.1.0
    # via
    #   aiohttp
    #   jsonschema
    #   referencing
backoff==2.2.1
    # via
    #   dspy
    #   litellm
beautifulsoup4==4.13.3
    # via
    #   medirag (pyproject.toml)
    #   llama-index-readers-file
boto3==1.36.24
    # via cohere
botocore==1.36.24
    # via
    #   boto3
    #   kdbai-client
    #   s3transfer
cachetools==5.5.1
    # via dspy
certifi==2025.1.31
    # via
    #   httpcore
    #   httpx
    #   requests
cffi==1.17.1
    # via
    #   cryptography
    #   pynacl
charset-normalizer==3.4.1
    # via requests
clean-text==0.6.0
    # via medirag (pyproject.toml)
click==8.1.8
    # via
    #   litellm
    #   nltk
    #   rq
    #   typer
    #   uvicorn
cloudpickle==3.1.1
    # via dspy
cohere==5.9.4
    # via kdbai-client
colorlog==6.9.0
    # via optuna
cryptography==43.0.3
    # via litellm
dataclasses-json==0.6.7
    # via llama-index-core
datasets==2.14.4
    # via dspy
decorator==5.1.1
    # via ipython
deprecated==1.2.18
    # via llama-index-core
dill==0.3.7
    # via
    #   datasets
    #   multiprocess
dirtyjson==1.0.8
    # via llama-index-core
diskcache==5.6.3
    # via dspy
distro==1.9.0
    # via openai
dnspython==2.7.0
    # via email-validator
dspy==2.6.5
    # via dspy-ai
dspy-ai==2.6.5
    # via medirag (pyproject.toml)
email-validator==2.2.0
    # via pydantic
emoji==1.7.0
    # via clean-text
executing==2.2.0
    # via stack-data
faiss-cpu==1.10.0
    # via medirag (pyproject.toml)
fastapi==0.115.8
    # via
    #   fastapi-sso
    #   gradio
    #   litellm
fastapi-sso==0.16.0
    # via litellm
fastavro==1.10.0
    # via cohere
ffmpy==0.5.0
    # via gradio
filelock==3.17.0
    # via
    #   huggingface-hub
    #   torch
    #   transformers
filetype==1.2.0
    # via llama-index-core
frozenlist==1.5.0
    # via
    #   aiohttp
    #   aiosignal
fsspec==2025.2.0
    # via
    #   datasets
    #   gradio-client
    #   huggingface-hub
    #   llama-index-core
    #   torch
ftfy==6.3.1
    # via clean-text
gradio==5.16.2
    # via medirag (pyproject.toml)
gradio-client==1.7.1
    # via gradio
greenlet==3.1.1
    # via sqlalchemy
gunicorn==22.0.0
    # via litellm
h11==0.14.0
    # via
    #   httpcore
    #   uvicorn
hatchling==1.27.0
    # via medirag (pyproject.toml)
httpcore==1.0.7
    # via httpx
httpx==0.28.1
    # via
    #   cohere
    #   dspy
    #   fastapi-sso
    #   gradio
    #   gradio-client
    #   langsmith
    #   litellm
    #   llama-index-core
    #   openai
    #   safehttpx
httpx-sse==0.4.0
    # via cohere
huggingface-hub==0.29.1
    # via
    #   accelerate
    #   datasets
    #   gradio
    #   gradio-client
    #   llama-index-embeddings-huggingface
    #   sentence-transformers
    #   tokenizers
    #   transformers
idna==3.10
    # via
    #   anyio
    #   email-validator
    #   httpx
    #   requests
    #   yarl
importlib-metadata==8.6.1
    # via litellm
ipython==8.32.0
    # via pyvis
jedi==0.19.2
    # via ipython
jinja2==3.1.5
    # via
    #   dspy
    #   gradio
    #   litellm
    #   pyvis
    #   torch
jiter==0.8.2
    # via openai
jmespath==1.0.1
    # via
    #   boto3
    #   botocore
joblib==1.4.2
    # via
    #   dspy
    #   nltk
    #   scikit-learn
json-repair==0.39.0
    # via dspy
jsonpatch==1.33
    # via langchain-core
jsonpickle==4.0.2
    # via pyvis
jsonpointer==3.0.0
    # via jsonpatch
jsonschema==4.23.0
    # via litellm
jsonschema-specifications==2024.10.1
    # via jsonschema
kdbai-client==1.6.0
    # via
    #   medirag (pyproject.toml)
    #   llama-index-vector-stores-kdbai
langchain==0.3.19
    # via medirag (pyproject.toml)
langchain-core==0.3.37
    # via
    #   langchain
    #   langchain-text-splitters
langchain-text-splitters==0.3.6
    # via langchain
langsmith==0.3.8
    # via
    #   langchain
    #   langchain-core
litellm==1.61.11
    # via dspy
llama-index-agent-openai==0.4.6
    # via medirag (pyproject.toml)
llama-index-core==0.12.19
    # via
    #   medirag (pyproject.toml)
    #   llama-index-agent-openai
    #   llama-index-embeddings-huggingface
    #   llama-index-embeddings-openai
    #   llama-index-llms-openai
    #   llama-index-readers-file
    #   llama-index-utils-workflow
    #   llama-index-vector-stores-faiss
    #   llama-index-vector-stores-kdbai
llama-index-embeddings-huggingface==0.5.1
    # via medirag (pyproject.toml)
llama-index-embeddings-openai==0.3.1
    # via medirag (pyproject.toml)
llama-index-llms-openai==0.3.20
    # via
    #   medirag (pyproject.toml)
    #   llama-index-agent-openai
llama-index-readers-file==0.4.5
    # via medirag (pyproject.toml)
llama-index-utils-workflow==0.3.0
    # via medirag (pyproject.toml)
llama-index-vector-stores-faiss==0.3.0
    # via medirag (pyproject.toml)
llama-index-vector-stores-kdbai==0.5.0
    # via medirag (pyproject.toml)
loguru==0.7.3
    # via medirag (pyproject.toml)
lxml==5.3.1
    # via medirag (pyproject.toml)
magicattr==0.1.6
    # via dspy
mako==1.3.9
    # via alembic
markdown-it-py==3.0.0
    # via rich
markupsafe==2.1.5
    # via
    #   gradio
    #   jinja2
    #   mako
marshmallow==3.26.1
    # via dataclasses-json
matplotlib-inline==0.1.7
    # via ipython
mdurl==0.1.2
    # via markdown-it-py
mpmath==1.3.0
    # via sympy
multidict==6.1.0
    # via
    #   aiohttp
    #   yarl
multiprocess==0.70.15
    # via datasets
mypy-extensions==1.0.0
    # via typing-inspect
nest-asyncio==1.6.0
    # via llama-index-core
networkx==3.4.2
    # via
    #   llama-index-core
    #   pyvis
    #   torch
nltk==3.9.1
    # via
    #   medirag (pyproject.toml)
    #   llama-index-core
numpy==1.26.4
    # via
    #   accelerate
    #   datasets
    #   faiss-cpu
    #   gradio
    #   langchain
    #   llama-index-core
    #   optuna
    #   pandas
    #   pykx
    #   scikit-learn
    #   scipy
    #   transformers
    #   voyageai
oauthlib==3.2.2
    # via fastapi-sso
openai==1.63.2
    # via
    #   dspy
    #   litellm
    #   llama-index-agent-openai
    #   llama-index-embeddings-openai
    #   llama-index-llms-openai
optuna==4.2.1
    # via dspy
orjson==3.10.15
    # via
    #   gradio
    #   langsmith
    #   litellm
packaging==24.2
    # via
    #   accelerate
    #   datasets
    #   faiss-cpu
    #   gradio
    #   gradio-client
    #   gunicorn
    #   hatchling
    #   huggingface-hub
    #   kdbai-client
    #   langchain-core
    #   marshmallow
    #   optuna
    #   transformers
pandas==2.1.4
    # via
    #   datasets
    #   dspy
    #   gradio
    #   kdbai-client
    #   llama-index-readers-file
    #   llama-index-vector-stores-kdbai
    #   pykx
parameterized==0.9.0
    # via cohere
parso==0.8.4
    # via jedi
pathspec==0.12.1
    # via hatchling
pexpect==4.9.0
    # via ipython
pillow==11.1.0
    # via
    #   gradio
    #   llama-index-core
    #   sentence-transformers
pluggy==1.5.0
    # via hatchling
prompt-toolkit==3.0.50
    # via ipython
propcache==0.2.1
    # via
    #   aiohttp
    #   yarl
psutil==7.0.0
    # via accelerate
ptyprocess==0.7.0
    # via pexpect
pure-eval==0.2.3
    # via stack-data
pyarrow==19.0.1
    # via datasets
pycparser==2.22
    # via cffi
pydantic==2.10.6
    # via
    #   cohere
    #   dspy
    #   fastapi
    #   fastapi-sso
    #   gradio
    #   langchain
    #   langchain-core
    #   langsmith
    #   litellm
    #   llama-index-core
    #   openai
pydantic-core==2.27.2
    # via
    #   cohere
    #   pydantic
pydub==0.25.1
    # via gradio
pygments==2.19.1
    # via
    #   ipython
    #   rich
pyjwt==2.10.1
    # via litellm
pykx==2.5.3
    # via
    #   medirag (pyproject.toml)
    #   kdbai-client
    #   llama-index-vector-stores-kdbai
pynacl==1.5.0
    # via litellm
pypdf==5.3.0
    # via llama-index-readers-file
python-dateutil==2.9.0.post0
    # via
    #   botocore
    #   pandas
python-dotenv==1.0.1
    # via
    #   medirag (pyproject.toml)
    #   litellm
python-multipart==0.0.18
    # via
    #   gradio
    #   litellm
pytz==2025.1
    # via
    #   pandas
    #   pykx
pyvis==0.3.2
    # via llama-index-utils-workflow
pyyaml==6.0.2
    # via
    #   accelerate
    #   datasets
    #   gradio
    #   huggingface-hub
    #   langchain
    #   langchain-core
    #   litellm
    #   llama-index-core
    #   optuna
    #   transformers
redis==5.2.1
    # via rq
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
regex==2024.11.6
    # via
    #   dspy
    #   nltk
    #   tiktoken
    #   transformers
requests==2.32.3
    # via
    #   cohere
    #   datasets
    #   dspy
    #   huggingface-hub
    #   kdbai-client
    #   langchain
    #   langsmith
    #   llama-index-core
    #   requests-toolbelt
    #   tiktoken
    #   transformers
    #   voyageai
requests-toolbelt==1.0.0
    # via langsmith
rich==13.9.4
    # via typer
rpds-py==0.23.0
    # via
    #   jsonschema
    #   referencing
rq==2.1.0
    # via litellm
ruff==0.9.7
    # via gradio
s3transfer==0.11.2
    # via boto3
safehttpx==0.1.6
    # via gradio
safetensors==0.5.2
    # via
    #   accelerate
    #   transformers
scikit-learn==1.6.1
    # via sentence-transformers
scipy==1.15.2
    # via
    #   scikit-learn
    #   sentence-transformers
semantic-version==2.10.0
    # via gradio
sentence-transformers==3.4.1
    # via
    #   medirag (pyproject.toml)
    #   llama-index-embeddings-huggingface
setuptools==75.8.0
    # via torch
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
sniffio==1.3.1
    # via
    #   anyio
    #   openai
soupsieve==2.6
    # via beautifulsoup4
sqlalchemy==2.0.38
    # via
    #   alembic
    #   langchain
    #   llama-index-core
    #   optuna
stack-data==0.6.3
    # via ipython
starlette==0.45.3
    # via
    #   fastapi
    #   gradio
striprtf==0.0.26
    # via llama-index-readers-file
sympy==1.13.1
    # via torch
tenacity==9.0.0
    # via
    #   dspy
    #   langchain
    #   langchain-core
    #   llama-index-core
    #   voyageai
threadpoolctl==3.5.0
    # via scikit-learn
tiktoken==0.9.0
    # via
    #   litellm
    #   llama-index-core
tokenizers==0.21.0
    # via
    #   cohere
    #   litellm
    #   transformers
toml==0.10.2
    # via pykx
tomlkit==0.13.2
    # via gradio
torch==2.6.0
    # via
    #   medirag (pyproject.toml)
    #   accelerate
    #   sentence-transformers
tqdm==4.67.1
    # via
    #   medirag (pyproject.toml)
    #   datasets
    #   dspy
    #   huggingface-hub
    #   llama-index-core
    #   nltk
    #   openai
    #   optuna
    #   sentence-transformers
    #   transformers
traitlets==5.14.3
    # via
    #   ipython
    #   matplotlib-inline
transformers==4.49.0
    # via
    #   medirag (pyproject.toml)
    #   sentence-transformers
trove-classifiers==2025.2.18.16
    # via hatchling
typer==0.15.1
    # via gradio
types-requests==2.32.0.20241016
    # via cohere
typing-extensions==4.12.2
    # via
    #   alembic
    #   anyio
    #   beautifulsoup4
    #   cohere
    #   fastapi
    #   gradio
    #   gradio-client
    #   huggingface-hub
    #   langchain-core
    #   llama-index-core
    #   openai
    #   pydantic
    #   pydantic-core
    #   referencing
    #   sqlalchemy
    #   torch
    #   typer
    #   typing-inspect
typing-inspect==0.9.0
    # via
    #   dataclasses-json
    #   llama-index-core
tzdata==2025.1
    # via pandas
tzlocal==5.3
    # via apscheduler
ujson==5.10.0
    # via dspy
urllib3==2.3.0
    # via
    #   botocore
    #   requests
    #   types-requests
uvicorn==0.29.0
    # via
    #   gradio
    #   litellm
uvloop==0.21.0
    # via litellm
voyageai==0.2.3
    # via kdbai-client
wcwidth==0.2.13
    # via
    #   ftfy
    #   prompt-toolkit
websockets==14.2
    # via gradio-client
wrapt==1.17.2
    # via
    #   deprecated
    #   llama-index-core
xxhash==3.5.0
    # via datasets
yarl==1.18.3
    # via aiohttp
zipp==3.21.0
    # via importlib-metadata
zstandard==0.23.0
    # via langsmith
